{"id":"vision","type":"class","name":"Vision","description":"<p class=\"notice\">  <strong>This is a Beta release of Google Cloud Vision.</strong> This API is not covered  by any SLA or deprecation policy and may be subject to backward-  incompatible changes. </p> <p>To learn more about the Vision API, see <a href=\"https://cloud.google.com/vision/docs/getting-started\">Getting Started</a>.</p>","source":"lib/vision/index.js","parent":null,"children":[],"methods":[{"id":"Vision","name":"Vision","type":"constructor","description":"<p>The <a href=\"https://cloud.google.com/vision/docs\">Cloud Vision API</a> allows easy integration of vision detection features, including image labeling, face and landmark detection, optical character recognition (OCR), and tagging of explicit content.</p>","source":"lib/vision/index.js#L96","resources":[{"title":"Getting Started","link":"https://cloud.google.com/vision/docs/getting-started"},{"title":"Image Best Practices","link":"https://cloud.google.com/vision/docs/image-best-practices"}],"examples":[{"code":"var gcloud = require('gcloud')({\n  keyFilename: '/path/to/keyfile.json',\n  projectId: 'grape-spaceship-123'\n});\n\nvar vision = gcloud.vision();"}],"params":[{"name":"options","description":"<ul> <li><a href=\"#/docs\">Configuration object</a>.</li> </ul> ","types":["object"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"annotate","name":"annotate","type":"instance","description":"<p>Run image detection and annotation for an image or batch of images.</p><p>This is an advanced API method that requires raw <a href=\"https://cloud.google.com/vision/reference/rest/v1/images/annotate#AnnotateImageRequest\"><code>AnnotateImageRequest</code></a> objects to be provided. If that doesn&#39;t sound like what you&#39;re looking for, you&#39;ll probably appreciate <a data-custom-type=\"vision\" data-method=\"detect\">vision#detect</a>.</p>","source":"lib/vision/index.js#L151","resources":[{"title":"images.annotate API Reference","link":"https://cloud.google.com/vision/reference/rest/v1/images/annotate"}],"examples":[{"code":"var annotateImageReq = {\n  // See the link in the parameters for `AnnotateImageRequest`.\n};\n\nvision.annotate(annotateImageReq, function(err, annotations, apiResponse) {\n  // annotations = apiResponse.responses\n});"}],"params":[{"name":"requests","description":"<ul> <li>An <code>AnnotateImageRequest</code> or array of <code>AnnotateImageRequest</code>s. See an  <a href=\"https://cloud.google.com/vision/reference/rest/v1/images/annotate#AnnotateImageRequest\"><code>AnnotateImageRequest</code></a>.</li> </ul> ","types":["object","object[]"],"optional":false,"nullable":false},{"name":"callback","description":"<ul> <li>The callback function.</li> </ul> ","types":["function"],"optional":false,"nullable":false},{"name":"callback.err","description":"<ul> <li>An error returned while making this request.</li> </ul> ","types":["error"],"optional":false,"nullable":true},{"name":"callback.annotations","description":"<ul> <li>See an <a href=\"https://cloud.google.com/vision/reference/rest/v1/images/annotate#AnnotateImageResponse\"><code>AnnotateImageResponse</code></a>.</li> </ul> ","types":["object"],"optional":false,"nullable":false},{"name":"callback.apiResponse","description":"<ul> <li>Raw API response.</li> </ul> ","types":["object"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"detect","name":"detect","type":"instance","description":"<p>Detect properties from an image (or images) of one or more types.</p><h4>API simplifications</h4> <p>The raw API response will return some values in a range from <code>VERY_UNLIKELY</code> to <code>VERY_LIKELY</code>. For simplification, any value less than <code>LIKELY</code> is converted to <code>false</code>.</p><ul> <li><strong>False</strong></li> <li><code>VERY_UNLIKELY</code></li> <li><code>UNLIKELY</code></li> <li><code>POSSIBLE</code></li> <li><strong>True</strong></li> <li><code>LIKELY</code></li> <li><code>VERY_LIKELY</code></li> </ul> <p>The API will also return many values represented in a <code>[0,1]</code> range. We convert these to a <code>[0,100]</code> value. E.g, <code>0.4</code> is represented as <code>40</code>.</p><p>For the response in the original format, review the <code>apiResponse</code> argument your callback receives.</p>","source":"lib/vision/index.js#L285","resources":[],"examples":[{"code":"var types = [\n  'face',\n  'label'\n];\n\nvision.detect('image.jpg', types, function(err, detections, apiResponse) {\n  // detections = {\n  //   faces: [...],\n  //   labels: [...]\n  // }\n});"},{"caption":"<p>Run feature detection over a remote image. \n*Note: This is not an officially supported feature of the Vision API. Our \nlibrary will make a request to the URL given, convert it to base64, and \nsend that upstream.*</p>","code":"var img = 'https://upload.wikimedia.org/wikipedia/commons/5/51/Google.png';\nvision.detect(img, types, function(err, detection, apiResponse) {});"},{"caption":"<p>Supply multiple images for feature detection.</p>","code":"var images = [\n  'image.jpg',\n  'image-two.jpg'\n];\n\nvar types = [\n  'face',\n  'label'\n];\n\nvision.detect(images, types, function(err, detections, apiResponse) {\n  // detections = [\n  //   // Detections for image.jpg:\n  //   {\n  //     faces: [...],\n  //     labels: [...]\n  //   },\n  //\n  //   // Detections for image-two.jpg:\n  //   {\n  //     faces: [...],\n  //     labels: [...]\n  //   }\n  // ]\n});"},{"caption":"<p>It's possible for part of your request to be completed successfully, while \na single feature request was not successful. Each returned detection will \nhave an <code>errors</code> array, including any of these errors which may have \noccurred.</p>","code":"vision.detect('malformed-image.jpg', types, function(err, detections) {\n  if (detections.faces.errors.length > 0) {\n    // Errors occurred while trying to use this image for a face annotation.\n  }\n});"}],"params":[{"name":"images","description":"<ul> <li>The source image(s) to run the detection on. It can be either a local image path, a remote image URL, or a gcloud File object.</li> </ul> ","types":["string","string[]","<a data-custom-type=\"storage/file\" data-method=\"\">storage/file</a>","<a data-custom-type=\"storage/file[]\" data-method=\"\">storage/file[]</a>"],"optional":false,"nullable":false},{"name":"options","description":"<ul> <li>An array of types or a configuration object.</li> </ul> ","types":["string[]","object"],"optional":true,"nullable":false},{"name":"options.imageContext","description":"<ul> <li>See an <a href=\"https://cloud.google.com/vision/reference/rest/v1/images/annotate#ImageContext\"><code>ImageContext</code></a>  resource.</li> </ul> ","types":["object"],"optional":true,"nullable":false},{"name":"options.maxResults","description":"<ul> <li>The maximum number of results, per type, to return in the response.</li> </ul> ","types":["number"],"optional":false,"nullable":false},{"name":"options.types","description":"<ul> <li>An array of feature types to detect from the provided images. Acceptable values: <code>faces</code>, <code>landmarks</code>, <code>labels</code>,  <code>logos</code>, <code>properties</code>, <code>safeSearch</code>, <code>text</code>.</li> </ul> ","types":["string[]"],"optional":false,"nullable":false},{"name":"options.verbose","description":"<ul> <li>Use verbose mode, which returns a less- simplistic representation of the annotation (default: <code>false</code>).</li> </ul> ","types":["boolean"],"optional":true,"nullable":false},{"name":"callback","description":"<ul> <li>The callback function.</li> </ul> ","types":["function"],"optional":false,"nullable":false},{"name":"callback.err","description":"<ul> <li>An error returned while making this request.</li> </ul> ","types":["error"],"optional":false,"nullable":true},{"name":"callback.detections","description":"<ul> <li>If a single detection type was asked for, it will be returned in its raw form; either an object or array  of objects. If multiple detection types were requested, you will receive  an object with keys for each detection type (listed above in  <code>config.types</code>). Additionally, if multiple images were provided, you will  receive an array of detection objects, each representing an image. See  the examples below for more information.</li> </ul> ","types":["object","object[]"],"optional":false,"nullable":false},{"name":"callback.detections.errors","description":"<ul> <li>It&#39;s possible for part of your request to be completed successfully, while a single feature request was  not successful. Each returned detection will have an <code>errors</code> array,  including any of these errors which may have occurred.</li> </ul> ","types":["object[]"],"optional":false,"nullable":false},{"name":"callback.apiResponse","description":"<ul> <li>Raw API response.</li> </ul> ","types":["object"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"detectFaces","name":"detectFaces","type":"instance","description":"<p>Run face detection against an image.</p><h4>Parameters</h4> <p>See <a data-custom-type=\"vision\" data-method=\"detect\">vision#detect</a>.</p>","source":"lib/vision/index.js#L890","resources":[{"title":"FaceAnnotation JSON respresentation","link":"https://cloud.google.com/vision/reference/rest/v1/images/annotate#FaceAnnotation"}],"examples":[{"code":"vision.detectFaces('image.jpg', function(err, faces, apiResponse) {\n  // faces = [\n  //   {\n  //     angles: {\n  //       pan: -8.1090336,\n  //       roll: -5.0002542,\n  //       tilt: 18.012161\n  //     },\n  //     bounds: {\n  //       head: [\n  //         {\n  //           x: 1\n  //         },\n  //         {\n  //           x: 295\n  //         },\n  //         {\n  //           x: 295,\n  //           y: 301\n  //         },\n  //         {\n  //           x: 1,\n  //           y: 301\n  //         }\n  //       ],\n  //       face: [\n  //         {\n  //           x: 28,\n  //           y: 40\n  //         },\n  //         {\n  //           x: 250,\n  //           y: 40\n  //         },\n  //         {\n  //           x: 250,\n  //           y: 262\n  //         },\n  //         {\n  //           x: 28,\n  //           y: 262\n  //         }\n  //       ]\n  //     },\n  //     features: {\n  //       confidence: 34.489909,\n  //       chin: {\n  //         center: {\n  //           x: 143.34183,\n  //           y: 262.22998,\n  //           z: -57.388493\n  //         },\n  //         left: {\n  //           x: 63.102425,\n  //           y: 248.99081,\n  //           z: 44.207638\n  //         },\n  //         right: {\n  //           x: 241.72728,\n  //           y: 225.53488,\n  //           z: 19.758242\n  //         }\n  //       },\n  //       ears: {\n  //         left: {\n  //           x: 54.872219,\n  //           y: 207.23712,\n  //           z: 97.030685\n  //         },\n  //         right: {\n  //           x: 252.67567,\n  //           y: 180.43124,\n  //           z: 70.15992\n  //         }\n  //       },\n  //       eyebrows: {\n  //         left: {\n  //           left: {\n  //             x: 58.790176,\n  //             y: 113.28249,\n  //             z: 17.89735\n  //           },\n  //           right: {\n  //             x: 106.14151,\n  //             y: 98.593758,\n  //             z: -13.116687\n  //           },\n  //           top: {\n  //             x: 80.248711,\n  //             y: 94.04303,\n  //             z: 0.21131183\n  //           }\n  //         },\n  //         right: {\n  //           left: {\n  //             x: 148.61565,\n  //             y: 92.294594,\n  //             z: -18.804882\n  //           },\n  //           right: {\n  //             x: 204.40808,\n  //             y: 94.300117,\n  //             z: -2.0009689\n  //           },\n  //           top: {\n  //             x: 174.70135,\n  //             y: 81.580917,\n  //             z: -12.702137\n  //           }\n  //         }\n  //       },\n  //       eyes: {\n  //         left: {\n  //           bottom: {\n  //             x: 84.883934,\n  //             y: 134.59479,\n  //             z: -2.8677137\n  //           },\n  //           center: {\n  //             x: 83.707092,\n  //             y: 128.34,\n  //             z: -0.00013388535\n  //           },\n  //           left: {\n  //             x: 72.213913,\n  //             y: 132.04138,\n  //             z: 9.6985674\n  //           },\n  //           pupil: {\n  //             x: 86.531624,\n  //             y: 126.49807,\n  //             z: -2.2496929\n  //           },\n  //           right: {\n  //             x: 105.28892,\n  //             y: 125.57655,\n  //             z: -2.51554\n  //           },\n  //           top: {\n  //             x: 86.706947,\n  //             y: 119.47144,\n  //             z: -4.1606765\n  //           }\n  //         },\n  //         right: {\n  //           bottom: {\n  //             x: 179.30353,\n  //             y: 121.03307,\n  //             z: -14.843414\n  //           },\n  //           center: {\n  //             x: 181.17694,\n  //             y: 115.16437,\n  //             z: -12.82961\n  //           },\n  //           left: {\n  //             x: 158.2863,\n  //             y: 118.491,\n  //             z: -9.723031\n  //           },\n  //           pupil: {\n  //             x: 175.99976,\n  //             y: 114.64407,\n  //             z: -14.53744\n  //           },\n  //           right: {\n  //             x: 194.59413,\n  //             y: 115.91954,\n  //             z: -6.952745\n  //           },\n  //           top: {\n  //             x: 173.99446,\n  //             y: 107.94287,\n  //             z: -16.050705\n  //           }\n  //         }\n  //       },\n  //       forehead: {\n  //         x: 126.53813,\n  //         y: 93.812057,\n  //         z: -18.863352\n  //       },\n  //       lips: {\n  //         bottom: {\n  //           x: 137.28528,\n  //           y: 219.23564,\n  //           z: -56.663128\n  //         },\n  //         top: {\n  //           x: 134.74164,\n  //           y: 192.50438,\n  //           z: -53.876408\n  //         }\n  //       },\n  //       mouth: {\n  //         center: {\n  //           x: 136.43481,\n  //           y: 204.37952,\n  //           z: -51.620205\n  //         },\n  //         left: {\n  //           x: 104.53558,\n  //           y: 214.05037,\n  //           z: -30.056231\n  //         },\n  //         right: {\n  //           x: 173.79134,\n  //           y: 204.99333,\n  //           z: -39.725758\n  //         }\n  //       },\n  //       nose: {\n  //         bottom: {\n  //           center: {\n  //             x: 133.81947,\n  //             y: 173.16437,\n  //             z: -48.287724\n  //           },\n  //           left: {\n  //             x: 110.98372,\n  //             y: 173.61331,\n  //             z: -29.7784\n  //           },\n  //           right: {\n  //             x: 161.31354,\n  //             y: 168.24527,\n  //             z: -36.1628\n  //           }\n  //         },\n  //         tip: {\n  //           x: 128.14919,\n  //           y: 153.68129,\n  //           z: -63.198204\n  //         },\n  //         top: {\n  //           x: 127.83745,\n  //           y: 110.17557,\n  //           z: -22.650913\n  //         }\n  //       }\n  //     },\n  //     confidence: 56.748849,\n  //     blurry: false,\n  //     dark: false,\n  //     happy: false,\n  //     hat: false,\n  //     mad: false,\n  //     sad: false,\n  //     surprised: false\n  //   }\n  // ]\n});"},{"caption":"<p>Our library simplifies the response from the API. Use the map below to see \neach response name's original name.</p>","code":"var shortNameToLongNameMap = {\n  chin: {\n    center: 'CHIN_GNATHION',\n    left: 'CHIN_LEFT_GONION',\n    right: 'CHIN_RIGHT_GONION'\n  },\n\n  ears: {\n    left: 'LEFT_EAR_TRAGION',\n    right: 'RIGHT_EAR_TRAGION'\n  },\n\n  eyebrows: {\n    left: {\n      left: 'LEFT_OF_LEFT_EYEBROW',\n      right: 'RIGHT_OF_LEFT_EYEBROW',\n      top: 'LEFT_EYEBROW_UPPER_MIDPOINT'\n    },\n    right: {\n      left: 'LEFT_OF_RIGHT_EYEBROW',\n      right: 'RIGHT_OF_RIGHT_EYEBROW',\n      top: 'RIGHT_EYEBROW_UPPER_MIDPOINT'\n    }\n  },\n\n  eyes: {\n    left: {\n      bottom: 'LEFT_EYE_BOTTOM_BOUNDARY',\n      center: 'LEFT_EYE',\n      left: 'LEFT_EYE_LEFT_CORNER',\n      pupil: 'LEFT_EYE_PUPIL',\n      right: 'LEFT_EYE_RIGHT_CORNER',\n      top: 'LEFT_EYE_TOP_BOUNDARY'\n    },\n    right: {\n      bottom: 'RIGHT_EYE_BOTTOM_BOUNDARY',\n      center: 'RIGHT_EYE',\n      left: 'RIGHT_EYE_LEFT_CORNER',\n      pupil: 'RIGHT_EYE_PUPIL',\n      right: 'RIGHT_EYE_RIGHT_CORNER',\n      top: 'RIGHT_EYE_TOP_BOUNDARY'\n    }\n  },\n\n  forehead: 'FOREHEAD_GLABELLA',\n\n  lips: {\n    bottom: 'LOWER_LIP',\n    top: 'UPPER_LIP'\n  },\n\n  mouth: {\n    center: 'MOUTH_CENTER',\n    left: 'MOUTH_LEFT',\n    right: 'MOUTH_RIGHT'\n  },\n\n  nose: {\n    bottom: {\n      center: 'NOSE_BOTTOM_CENTER',\n      left: 'NOSE_BOTTOM_LEFT',\n      right: 'NOSE_BOTTOM_RIGHT'\n    },\n    tip: 'NOSE_TIP',\n    top: 'MIDPOINT_BETWEEN_EYES'\n  }\n};"}],"params":[],"exceptions":[],"returns":[]},{"id":"detectLabels","name":"detectLabels","type":"instance","description":"<p>Annotate an image with descriptive labels.</p><h4>Parameters</h4> <p>See <a data-custom-type=\"vision\" data-method=\"detect\">vision#detect</a>.</p>","source":"lib/vision/index.js#L946","resources":[{"title":"EntityAnnotation JSON representation","link":"https://cloud.google.com/vision/reference/rest/v1/images/annotate#EntityAnnotation"}],"examples":[{"code":"vision.detectLabels('image.jpg', function(err, labels, apiResponse) {\n  // labels = [\n  //   'classical sculpture',\n  //   'statue',\n  //   'landmark',\n  //   'ancient history',\n  //   'artwork'\n  // ]\n});"},{"caption":"<p>Activate <code>verbose</code> mode for a more detailed response.</p>","code":"var opts = {\n  verbose: true\n};\n\nvision.detectLabels('image.jpg', opts, function(err, labels, apiResponse) {\n  // labels = [\n  //   {\n  //     desc: 'classical sculpture',\n  //     id: '/m/095yjj',\n  //     score: 98.092282\n  //   },\n  //   {\n  //     desc: 'statue',\n  //     id: '/m/013_1c',\n  //     score: 90.66112\n  //   },\n  //   // ...\n  // ]\n});"}],"params":[],"exceptions":[],"returns":[]},{"id":"detectLandmarks","name":"detectLandmarks","type":"instance","description":"<p>Detect the landmarks from an image.</p><h4>Parameters</h4> <p>See <a data-custom-type=\"vision\" data-method=\"detect\">vision#detect</a>.</p>","source":"lib/vision/index.js#L1018","resources":[{"title":"EntityAnnotation JSON representation","link":"https://cloud.google.com/vision/reference/rest/v1/images/annotate#EntityAnnotation"}],"examples":[{"code":"vision.detectLandmarks('image.jpg', function(err, landmarks, apiResponse) {\n  // landmarks = [\n  //   'Mount Rushmore'\n  // ]\n});"},{"caption":"<p>Activate <code>verbose</code> mode for a more detailed response.</p>","code":"var image = 'image.jpg';\n\nvar opts = {\n  verbose: true\n};\n\nvision.detectLandmarks(image, opts, function(err, landmarks, apiResponse) {\n  // landmarks = [\n  //   {\n  //     desc: 'Mount Rushmore',\n  //     id: '/m/019dvv',\n  //     score: 28.651705,\n  //     bounds: [\n  //       {\n  //         x: 79,\n  //         y: 130\n  //       },\n  //       {\n  //         x: 284,\n  //         y: 130\n  //       },\n  //       {\n  //         x: 284,\n  //         y: 226\n  //       },\n  //       {\n  //         x: 79,\n  //         y: 226\n  //       }\n  //     ],\n  //     locations: [\n  //       {\n  //         latitude: 43.878264,\n  //         longitude: -103.45700740814209\n  //       }\n  //     ]\n  //   }\n  // ]\n});"}],"params":[],"exceptions":[],"returns":[]},{"id":"detectLogos","name":"detectLogos","type":"instance","description":"<p>Detect the logos from an image.</p><h4>Parameters</h4> <p>See <a data-custom-type=\"vision\" data-method=\"detect\">vision#detect</a>.</p>","source":"lib/vision/index.js#L1082","resources":[{"title":"EntityAnnotation JSON representation","link":"https://cloud.google.com/vision/reference/rest/v1/images/annotate#EntityAnnotation"}],"examples":[{"code":"vision.detectLogos('image.jpg', function(err, logos, apiResponse) {\n  // logos = [\n  //   'Google'\n  // ]\n});"},{"caption":"<p>Activate <code>verbose</code> mode for a more detailed response.</p>","code":"var options = {\n  verbose: true\n};\n\nvision.detectLogos('image.jpg', options, function(err, logos, apiResponse) {\n  // logos = [\n  //   {\n  //     desc: 'Google',\n  //     id: '/m/045c7b',\n  //     score: 64.35439,\n  //     bounds: [\n  //       {\n  //         x: 11,\n  //         y: 11\n  //       },\n  //       {\n  //         x: 330,\n  //         y: 11\n  //       },\n  //       {\n  //         x: 330,\n  //         y: 72\n  //       },\n  //       {\n  //         x: 11,\n  //         y: 72\n  //       }\n  //     ]\n  //   }\n  // ]\n});"}],"params":[],"exceptions":[],"returns":[]},{"id":"detectProperties","name":"detectProperties","type":"instance","description":"<p>Get a set of properties about an image, such as its dominant colors.</p><h4>Parameters</h4> <p>See <a data-custom-type=\"vision\" data-method=\"detect\">vision#detect</a>.</p>","source":"lib/vision/index.js#L1155","resources":[{"title":"ImageProperties JSON representation","link":"https://cloud.google.com/vision/reference/rest/v1/images/annotate#ImageProperties"}],"examples":[{"code":"vision.detectProperties('image.jpg', function(err, props, apiResponse) {\n  // props = {\n  //   colors: [\n  //     '3b3027',\n  //     '727d81',\n  //     '3f2f22',\n  //     '838e92',\n  //     '482a16',\n  //     '5f4f3c',\n  //     '261b14',\n  //     'b39b7f',\n  //     '51473f',\n  //     '2c1e12'\n  //   ]\n  // }\n});"},{"caption":"<p>Activate <code>verbose</code> mode for a more detailed response.</p>","code":"var image = 'image.jpg';\n\nvar options = {\n  verbose: true\n};\n\nvision.detectProperties(image, options, function(err, props, apiResponse) {\n  // props = {\n  //   colors: [\n  //     {\n  //       red: 59,\n  //       green: 48,\n  //       blue: 39,\n  //       score: 26.618013,\n  //       coverage: 15.948276,\n  //       hex: '3b3027'\n  //     },\n  //     {\n  //       red: 114,\n  //       green: 125,\n  //       blue: 129,\n  //       score: 10.319714,\n  //       coverage: 8.3977409,\n  //       hex: '727d81'\n  //     },\n  //     // ...\n  //   ]\n  // }\n});"}],"params":[],"exceptions":[],"returns":[]},{"id":"detectSafeSearch","name":"detectSafeSearch","type":"instance","description":"<p>Detect the SafeSearch flags from an image.</p><h4>Parameters</h4> <p>See <a data-custom-type=\"vision\" data-method=\"detect\">vision#detect</a>.</p>","source":"lib/vision/index.js#L1187","resources":[{"title":"SafeSearch JSON representation","link":"https://cloud.google.com/vision/reference/rest/v1/images/annotate#SafeSearchAnnotation"}],"examples":[{"code":"vision.detectSafeSearch('image.jpg', function(err, safeSearch, apiResponse) {\n  // safeSearch = {\n  //   adult: false,\n  //   medical: false,\n  //   spoof: false,\n  //   violence: true\n  // }\n});"}],"params":[],"exceptions":[],"returns":[]},{"id":"detectText","name":"detectText","type":"instance","description":"<p>Detect the text within an image.</p><h4>Parameters</h4> <p>See <a data-custom-type=\"vision\" data-method=\"detect\">vision#detect</a>.</p>","source":"lib/vision/index.js#L1247","resources":[],"examples":[{"code":"vision.detectText('image.jpg', function(err, text, apiResponse) {\n  // text = [\n  //   'This was text found in the image'\n  // ]\n});"},{"caption":"<p>Activate <code>verbose</code> mode for a more detailed response.</p>","code":"var options = {\n  verbose: true\n};\n\nvision.detectText('image.jpg', options, function(err, text, apiResponse) {\n  // text = [\n  //   {\n  //     desc: 'This was text found in the image',\n  //     bounds: [\n  //       {\n  //          x: 4,\n  //          y: 5\n  //       },\n  //       {\n  //          x: 493,\n  //          y: 5\n  //       },\n  //       {\n  //          x: 493,\n  //          y: 89\n  //       },\n  //       {\n  //          x: 4,\n  //          y: 89\n  //       }\n  //     ]\n  //   }\n  // ]\n});"}],"params":[],"exceptions":[],"returns":[]}]}