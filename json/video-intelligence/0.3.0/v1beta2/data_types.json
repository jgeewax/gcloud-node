{"name":"Data Types","methods":[{"type":"instance","description":"<p><code>Any</code> contains an arbitrary serialized protocol buffer message along with a URL that describes the type of the serialized message.</p><p>Protobuf library provides support to pack/unpack Any values in the form of utility functions or additional generated methods of the Any type.</p><p>Example 1: Pack and unpack a message in C++.</p><pre><code>Foo foo = ...; Any any; any.PackFrom(foo); ... if (any.UnpackTo(&amp;foo)) { ... } </code></pre><p>Example 2: Pack and unpack a message in Java.</p><pre><code>Foo foo = ...; Any any = Any.pack(foo); ... if (any.is(Foo.class)) { foo = any.unpack(Foo.class); } </code></pre><p> Example 3: Pack and unpack a message in Python.</p><pre><code>foo = Foo(...) any = Any() any.Pack(foo) ... if any.Is(Foo.DESCRIPTOR): any.Unpack(foo) ... </code></pre><p> Example 4: Pack and unpack a message in Go</p><pre><code> foo := &amp;pb.Foo{...} any, err := ptypes.MarshalAny(foo) ... foo := &amp;pb.Foo{} if err := ptypes.UnmarshalAny(any, foo); err != nil { ... } </code></pre><p>The pack methods provided by protobuf library will by default use &#39;type.googleapis.com/full.type.name&#39; as the type URL and the unpack methods only use the fully qualified type name after the last &#39;/&#39; in the type URL, for example &quot;foo.bar.com/x/y.z&quot; will yield type name &quot;y.z&quot;.</p><h1>JSON</h1> <p>The JSON representation of an <code>Any</code> value uses the regular representation of the deserialized, embedded message, with an additional field <code>@type</code> which contains the type URL. Example:</p><pre><code>package google.profile; message Person { string first_name = 1; string last_name = 2; } { &quot;@type&quot;: &quot;type.googleapis.com/google.profile.Person&quot;, &quot;firstName&quot;: &lt;string&gt;, &quot;lastName&quot;: &lt;string&gt; } </code></pre><p>If the embedded message type is well-known and has a custom JSON representation, that representation will be embedded adding a field <code>value</code> which holds the custom JSON in addition to the <code>@type</code> field. Example (for message {@link google.protobuf.Duration}):</p><pre><code>{ &quot;@type&quot;: &quot;type.googleapis.com/google.protobuf.Duration&quot;, &quot;value&quot;: &quot;1.212s&quot; } </code></pre>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_google_protobuf_any.js#L132","resources":[],"examples":[],"params":[{"name":"typeUrl","description":"<p> A URL/resource name whose content describes the type of the  serialized protocol buffer message.</p><p> For URLs which use the scheme <code>http</code>, <code>https</code>, or no scheme, the  following restrictions and interpretations apply:</p><ul> <li>If no scheme is provided, <code>https</code> is assumed.</li> <li>The last segment of the URL&#39;s path must represent the fully qualified name of the type (as in <code>path/google.protobuf.Duration</code>). The name should be in a canonical form (e.g., leading &quot;.&quot; is not accepted).</li> <li>An HTTP GET on the URL must yield a {@link google.protobuf.Type} value in binary format, or produce an error.</li> <li><p>Applications are allowed to cache lookup results based on the URL, or have them precompiled into a binary to avoid any lookup. Therefore, binary compatibility needs to be preserved on changes to types. (Use versioned type names to manage breaking changes.)</p><p>Schemes other than <code>http</code>, <code>https</code> (or the empty scheme) might be used with implementation specific semantics.</p></li> </ul> ","types":["string"],"optional":false,"nullable":false},{"name":"value","description":"<p> Must be a valid serialized protocol buffer of the above specified type.</p>","types":["string"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"type":"instance","description":"<p>A Duration represents a signed, fixed-length span of time represented as a count of seconds and fractions of seconds at nanosecond resolution. It is independent of any calendar and concepts like &quot;day&quot; or &quot;month&quot;. It is related to Timestamp in that the difference between two Timestamp values is a Duration and it can be added or subtracted from a Timestamp. Range is approximately +-10,000 years.</p><h1>Examples</h1> <p>Example 1: Compute Duration from two Timestamps in pseudo code.</p><pre><code>Timestamp start = ...; Timestamp end = ...; Duration duration = ...; duration.seconds = end.seconds - start.seconds; duration.nanos = end.nanos - start.nanos; if (duration.seconds &lt; 0 &amp;&amp; duration.nanos &gt; 0) { duration.seconds += 1; duration.nanos -= 1000000000; } else if (durations.seconds &gt; 0 &amp;&amp; duration.nanos &lt; 0) { duration.seconds -= 1; duration.nanos += 1000000000; } </code></pre><p>Example 2: Compute Timestamp from Timestamp + Duration in pseudo code.</p><pre><code>Timestamp start = ...; Duration duration = ...; Timestamp end = ...; end.seconds = start.seconds + duration.seconds; end.nanos = start.nanos + duration.nanos; if (end.nanos &lt; 0) { end.seconds -= 1; end.nanos += 1000000000; } else if (end.nanos &gt;= 1000000000) { end.seconds += 1; end.nanos -= 1000000000; } </code></pre><p>Example 3: Compute Duration from datetime.timedelta in Python.</p><pre><code>td = datetime.timedelta(days=3, minutes=10) duration = Duration() duration.FromTimedelta(td) </code></pre><h1>JSON Mapping</h1> <p>In JSON format, the Duration type is encoded as a string rather than an object, where the string ends in the suffix &quot;s&quot; (indicating seconds) and is preceded by the number of seconds, with nanoseconds expressed as fractional seconds. For example, 3 seconds with 0 nanoseconds should be encoded in JSON format as &quot;3s&quot;, while 3 seconds and 1 nanosecond should be expressed in JSON format as &quot;3.000000001s&quot;, and 3 seconds and 1 microsecond should be expressed in JSON format as &quot;3.000001s&quot;.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_google_protobuf_duration.js#L98","resources":[],"examples":[],"params":[{"name":"seconds","description":"<p> Signed seconds of the span of time. Must be from -315,576,000,000  to +315,576,000,000 inclusive. Note: these bounds are computed from:  60 sec/min <em> 60 min/hr </em> 24 hr/day <em> 365.25 days/year </em> 10000 years</p>","types":["number"],"optional":false,"nullable":false},{"name":"nanos","description":"<p> Signed fractions of a second at nanosecond resolution of the span  of time. Durations less than one second are represented with a 0  <code>seconds</code> field and a positive or negative <code>nanos</code> field. For durations  of one second or more, a non-zero value for the <code>nanos</code> field must be  of the same sign as the <code>seconds</code> field. Must be from -999,999,999  to +999,999,999 inclusive.</p>","types":["number"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"type":"instance","description":"<p>The <code>Status</code> type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. It is used by <a href=\"https://github.com/grpc\">gRPC</a>. The error model is designed to be:</p><ul> <li>Simple to use and understand for most users</li> <li>Flexible enough to meet unexpected needs</li> </ul> <h1>Overview</h1> <p>The <code>Status</code> message contains three pieces of data: error code, error message, and error details. The error code should be an enum value of {@link google.rpc.Code}, but it may accept additional error codes if needed. The error message should be a developer-facing English message that helps developers <em>understand</em> and <em>resolve</em> the error. If a localized user-facing error message is needed, put the localized message in the error details or localize it in the client. The optional error details may contain arbitrary information about the error. There is a predefined set of error detail types in the package <code>google.rpc</code> that can be used for common error conditions.</p><h1>Language mapping</h1> <p>The <code>Status</code> message is the logical representation of the error model, but it is not necessarily the actual wire format. When the <code>Status</code> message is exposed in different client libraries and different wire protocols, it can be mapped differently. For example, it will likely be mapped to some exceptions in Java, but more likely mapped to some error codes in C.</p><h1>Other uses</h1> <p>The error model and the <code>Status</code> message can be used in a variety of environments, either with or without APIs, to provide a consistent developer experience across different environments.</p><p>Example uses of this error model include:</p><ul> <li><p>Partial errors. If a service needs to return partial errors to the client,  it may embed the <code>Status</code> in the normal response to indicate the partial  errors.</p></li> <li><p>Workflow errors. A typical workflow has multiple steps. Each step may  have a <code>Status</code> message for error reporting.</p></li> <li><p>Batch operations. If a client uses batch request and batch response, the  <code>Status</code> message should be used directly inside batch response, one for  each error sub-response.</p></li> <li><p>Asynchronous operations. If an API call embeds asynchronous operation  results in its response, the status of those operations should be  represented directly using the <code>Status</code> message.</p></li> <li><p>Logging. If some API errors are stored in logs, the message <code>Status</code> could  be used directly after any stripping needed for security/privacy reasons.</p></li> </ul> ","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_google_rpc_status.js#L93","resources":[],"examples":[],"params":[{"name":"code","description":"<p> The status code, which should be an enum value of {@link google.rpc.Code}.</p>","types":["number"],"optional":false,"nullable":false},{"name":"message","description":"<p> A developer-facing error message, which should be in English. Any  user-facing error message should be localized and sent in the  {@link google.rpc.Status.details} field, or localized by the client.</p>","types":["string"],"optional":false,"nullable":false},{"name":"details","description":"<p> A list of messages that carry the error details. There is a common set of  message types for APIs to use.</p><p> This object should have the same structure as google.protobuf.Any</p>","types":["Object[]"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"AnnotateVideoRequest","name":"AnnotateVideoRequest","type":"instance","description":"","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L67","resources":[],"examples":[],"params":[{"name":"inputUri","description":"<p> Input video location. Currently, only  <a href=\"https://cloud.google.com/storage/\">Google Cloud Storage</a> URIs are  supported, which must be specified in the following format:  <code>gs://bucket-id/object-id</code> (other URI formats return  {@link google.rpc.Code.INVALID_ARGUMENT}). For more information, see  <a href=\"https://cloud.google.com/storage/docs/reference-uris\">Request URIs</a>.  A video URI may include wildcards in <code>object-id</code>, and thus identify  multiple videos. Supported wildcards: &#39;*&#39; to match 0 or more characters;  &#39;?&#39; to match 1 character. If unset, the input video should be embedded  in the request as <code>input_content</code>. If set, <code>input_content</code> should be unset.</p>","types":["string"],"optional":false,"nullable":false},{"name":"inputContent","description":"<p> The video data bytes. Encoding: base64. If unset, the input video(s)  should be specified via <code>input_uri</code>. If set, <code>input_uri</code> should be unset.</p>","types":["string"],"optional":false,"nullable":false},{"name":"features","description":"<p> Requested video annotation features.</p><p> The number should be among the values of <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'Feature'\n        })\">Feature</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'Feature'\n        })\">Feature</a></p>","types":["number[]"],"optional":false,"nullable":false},{"name":"videoContext","description":"<p> Additional video context and/or feature-specific parameters.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'VideoContext'\n        })\">VideoContext</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'VideoContext'\n        })\">VideoContext</a></p>","types":["Object"],"optional":false,"nullable":false},{"name":"outputUri","description":"<p> Optional location where the output (in JSON format) should be stored.  Currently, only <a href=\"https://cloud.google.com/storage/\">Google Cloud Storage</a>  URIs are supported, which must be specified in the following format:  <code>gs://bucket-id/object-id</code> (other URI formats return  {@link google.rpc.Code.INVALID_ARGUMENT}). For more information, see  <a href=\"https://cloud.google.com/storage/docs/reference-uris\">Request URIs</a>.</p>","types":["string"],"optional":false,"nullable":false},{"name":"locationId","description":"<p> Optional cloud region where annotation should take place. Supported cloud  regions: <code>us-east1</code>, <code>us-west1</code>, <code>europe-west1</code>, <code>asia-east1</code>. If no region  is specified, a region will be determined based on video file location.</p>","types":["string"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"VideoContext","name":"VideoContext","type":"instance","description":"<p>Video context and/or feature-specific parameters.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L104","resources":[],"examples":[],"params":[{"name":"segments","description":"<p> Video segments to annotate. The segments may overlap and are not required  to be contiguous or span the whole video. If unspecified, each video  is treated as a single segment.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'VideoSegment'\n        })\">VideoSegment</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'VideoSegment'\n        })\">VideoSegment</a></p>","types":["Object[]"],"optional":false,"nullable":false},{"name":"labelDetectionConfig","description":"<p> Config for LABEL_DETECTION.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'LabelDetectionConfig'\n        })\">LabelDetectionConfig</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'LabelDetectionConfig'\n        })\">LabelDetectionConfig</a></p>","types":["Object"],"optional":false,"nullable":false},{"name":"shotChangeDetectionConfig","description":"<p> Config for SHOT_CHANGE_DETECTION.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'ShotChangeDetectionConfig'\n        })\">ShotChangeDetectionConfig</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'ShotChangeDetectionConfig'\n        })\">ShotChangeDetectionConfig</a></p>","types":["Object"],"optional":false,"nullable":false},{"name":"explicitContentDetectionConfig","description":"<p> Config for EXPLICIT_CONTENT_DETECTION.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'ExplicitContentDetectionConfig'\n        })\">ExplicitContentDetectionConfig</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'ExplicitContentDetectionConfig'\n        })\">ExplicitContentDetectionConfig</a></p>","types":["Object"],"optional":false,"nullable":false},{"name":"faceDetectionConfig","description":"<p> Config for FACE_DETECTION.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'FaceDetectionConfig'\n        })\">FaceDetectionConfig</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'FaceDetectionConfig'\n        })\">FaceDetectionConfig</a></p>","types":["Object"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"LabelDetectionConfig","name":"LabelDetectionConfig","type":"instance","description":"<p>Config for LABEL_DETECTION.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L131","resources":[],"examples":[],"params":[{"name":"labelDetectionMode","description":"<p> What labels should be detected with LABEL_DETECTION, in addition to  video-level labels or segment-level labels.  If unspecified, defaults to <code>SHOT_MODE</code>.</p><p> The number should be among the values of <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'LabelDetectionMode'\n        })\">LabelDetectionMode</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'LabelDetectionMode'\n        })\">LabelDetectionMode</a></p>","types":["number"],"optional":false,"nullable":false},{"name":"stationaryCamera","description":"<p> Whether the video has been shot from a stationary (i.e. non-moving) camera.  When set to true, might improve detection accuracy for moving objects.  Should be used with <code>SHOT_AND_FRAME_MODE</code> enabled.</p>","types":["boolean"],"optional":false,"nullable":false},{"name":"model","description":"<p> Model to use for label detection.  Supported values: &quot;builtin/stable&quot; (the default if unset) and  &quot;builtin/latest&quot;.</p>","types":["string"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"ShotChangeDetectionConfig","name":"ShotChangeDetectionConfig","type":"instance","description":"<p>Config for SHOT_CHANGE_DETECTION.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L146","resources":[],"examples":[],"params":[{"name":"model","description":"<p> Model to use for shot change detection.  Supported values: &quot;builtin/stable&quot; (the default if unset) and  &quot;builtin/latest&quot;.</p>","types":["string"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"ExplicitContentDetectionConfig","name":"ExplicitContentDetectionConfig","type":"instance","description":"<p>Config for EXPLICIT_CONTENT_DETECTION.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L161","resources":[],"examples":[],"params":[{"name":"model","description":"<p> Model to use for explicit content detection.  Supported values: &quot;builtin/stable&quot; (the default if unset) and  &quot;builtin/latest&quot;.</p>","types":["string"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"FaceDetectionConfig","name":"FaceDetectionConfig","type":"instance","description":"<p>Config for FACE_DETECTION.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L179","resources":[],"examples":[],"params":[{"name":"model","description":"<p> Model to use for face detection.  Supported values: &quot;builtin/stable&quot; (the default if unset) and  &quot;builtin/latest&quot;.</p>","types":["string"],"optional":false,"nullable":false},{"name":"includeBoundingBoxes","description":"<p> Whether bounding boxes be included in the face annotation output.</p>","types":["boolean"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"VideoSegment","name":"VideoSegment","type":"instance","description":"<p>Video segment.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L201","resources":[],"examples":[],"params":[{"name":"startTimeOffset","description":"<p> Time-offset, relative to the beginning of the video,  corresponding to the start of the segment (inclusive).</p><p> This object should have the same structure as google.protobuf.Duration</p>","types":["Object"],"optional":false,"nullable":false},{"name":"endTimeOffset","description":"<p> Time-offset, relative to the beginning of the video,  corresponding to the end of the segment (inclusive).</p><p> This object should have the same structure as google.protobuf.Duration</p>","types":["Object"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"LabelSegment","name":"LabelSegment","type":"instance","description":"<p>Video segment level annotation results for label detection.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L219","resources":[],"examples":[],"params":[{"name":"segment","description":"<p> Video segment where a label was detected.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'VideoSegment'\n        })\">VideoSegment</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'VideoSegment'\n        })\">VideoSegment</a></p>","types":["Object"],"optional":false,"nullable":false},{"name":"confidence","description":"<p> Confidence that the label is accurate. Range: [0, 1].</p>","types":["number"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"LabelFrame","name":"LabelFrame","type":"instance","description":"<p>Video frame level annotation results for label detection.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L238","resources":[],"examples":[],"params":[{"name":"timeOffset","description":"<p> Time-offset, relative to the beginning of the video, corresponding to the  video frame for this location.</p><p> This object should have the same structure as google.protobuf.Duration</p>","types":["Object"],"optional":false,"nullable":false},{"name":"confidence","description":"<p> Confidence that the label is accurate. Range: [0, 1].</p>","types":["number"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"Entity","name":"Entity","type":"instance","description":"<p>Detected entity from video analysis.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L259","resources":[],"examples":[],"params":[{"name":"entityId","description":"<p> Opaque entity ID. Some IDs may be available in  <a href=\"https://developers.google.com/knowledge-graph/\">Google Knowledge Graph Search  API</a>.</p>","types":["string"],"optional":false,"nullable":false},{"name":"description","description":"<p> Textual description, e.g. <code>Fixed-gear bicycle</code>.</p>","types":["string"],"optional":false,"nullable":false},{"name":"languageCode","description":"<p> Language code for <code>description</code> in BCP-47 format.</p>","types":["string"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"LabelAnnotation","name":"LabelAnnotation","type":"instance","description":"<p>Label annotation.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L292","resources":[],"examples":[],"params":[{"name":"entity","description":"<p> Detected entity.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'Entity'\n        })\">Entity</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'Entity'\n        })\">Entity</a></p>","types":["Object"],"optional":false,"nullable":false},{"name":"categoryEntities","description":"<p> Common categories for the detected entity.  E.g. when the label is <code>Terrier</code> the category is likely <code>dog</code>. And in some  cases there might be more than one categories e.g. <code>Terrier</code> could also be  a <code>pet</code>.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'Entity'\n        })\">Entity</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'Entity'\n        })\">Entity</a></p>","types":["Object[]"],"optional":false,"nullable":false},{"name":"segments","description":"<p> All video segments where a label was detected.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'LabelSegment'\n        })\">LabelSegment</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'LabelSegment'\n        })\">LabelSegment</a></p>","types":["Object[]"],"optional":false,"nullable":false},{"name":"frames","description":"<p> All video frames where a label was detected.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'LabelFrame'\n        })\">LabelFrame</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'LabelFrame'\n        })\">LabelFrame</a></p>","types":["Object[]"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"ExplicitContentFrame","name":"ExplicitContentFrame","type":"instance","description":"<p>Video frame level annotation results for explicit content.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L313","resources":[],"examples":[],"params":[{"name":"timeOffset","description":"<p> Time-offset, relative to the beginning of the video, corresponding to the  video frame for this location.</p><p> This object should have the same structure as google.protobuf.Duration</p>","types":["Object"],"optional":false,"nullable":false},{"name":"pornographyLikelihood","description":"<p> Likelihood of the pornography content..</p><p> The number should be among the values of <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'Likelihood'\n        })\">Likelihood</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'Likelihood'\n        })\">Likelihood</a></p>","types":["number"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"ExplicitContentAnnotation","name":"ExplicitContentAnnotation","type":"instance","description":"<p>Explicit content annotation (based on per-frame visual signals only). If no explicit content has been detected in a frame, no annotations are present for that frame.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L330","resources":[],"examples":[],"params":[{"name":"frames","description":"<p> All video frames where explicit content was detected.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'ExplicitContentFrame'\n        })\">ExplicitContentFrame</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'ExplicitContentFrame'\n        })\">ExplicitContentFrame</a></p>","types":["Object[]"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"NormalizedBoundingBox","name":"NormalizedBoundingBox","type":"instance","description":"<p>Normalized bounding box. The normalized vertex coordinates are relative to the original image. Range: [0, 1].</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L354","resources":[],"examples":[],"params":[{"name":"left","description":"<p> Left X coordinate.</p>","types":["number"],"optional":false,"nullable":false},{"name":"top","description":"<p> Top Y coordinate.</p>","types":["number"],"optional":false,"nullable":false},{"name":"right","description":"<p> Right X coordinate.</p>","types":["number"],"optional":false,"nullable":false},{"name":"bottom","description":"<p> Bottom Y coordinate.</p>","types":["number"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"FaceSegment","name":"FaceSegment","type":"instance","description":"<p>Video segment level annotation results for face detection.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L369","resources":[],"examples":[],"params":[{"name":"segment","description":"<p> Video segment where a face was detected.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'VideoSegment'\n        })\">VideoSegment</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'VideoSegment'\n        })\">VideoSegment</a></p>","types":["Object"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"FaceFrame","name":"FaceFrame","type":"instance","description":"<p>Video frame level annotation results for face detection.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L392","resources":[],"examples":[],"params":[{"name":"normalizedBoundingBoxes","description":"<p> Normalized Bounding boxes in a frame.  There can be more than one boxes if the same face is detected in multiple  locations within the current frame.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'NormalizedBoundingBox'\n        })\">NormalizedBoundingBox</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'NormalizedBoundingBox'\n        })\">NormalizedBoundingBox</a></p>","types":["Object[]"],"optional":false,"nullable":false},{"name":"timeOffset","description":"<p> Time-offset, relative to the beginning of the video,  corresponding to the video frame for this location.</p><p> This object should have the same structure as google.protobuf.Duration</p>","types":["Object"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"FaceAnnotation","name":"FaceAnnotation","type":"instance","description":"<p>Face annotation.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L415","resources":[],"examples":[],"params":[{"name":"thumbnail","description":"<p> Thumbnail of a representative face view (in JPEG format). Encoding: base64.</p>","types":["string"],"optional":false,"nullable":false},{"name":"segments","description":"<p> All video segments where a face was detected.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'FaceSegment'\n        })\">FaceSegment</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'FaceSegment'\n        })\">FaceSegment</a></p>","types":["Object[]"],"optional":false,"nullable":false},{"name":"frames","description":"<p> All video frames where a face was detected.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'FaceFrame'\n        })\">FaceFrame</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'FaceFrame'\n        })\">FaceFrame</a></p>","types":["Object[]"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"VideoAnnotationResults","name":"VideoAnnotationResults","type":"instance","description":"<p>Annotation results for a single video.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L468","resources":[],"examples":[],"params":[{"name":"inputUri","description":"<p> Video file location in  <a href=\"https://cloud.google.com/storage/\">Google Cloud Storage</a>.</p>","types":["string"],"optional":false,"nullable":false},{"name":"segmentLabelAnnotations","description":"<p> Label annotations on video level or user specified segment level.  There is exactly one element for each unique label.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'LabelAnnotation'\n        })\">LabelAnnotation</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'LabelAnnotation'\n        })\">LabelAnnotation</a></p>","types":["Object[]"],"optional":false,"nullable":false},{"name":"shotLabelAnnotations","description":"<p> Label annotations on shot level.  There is exactly one element for each unique label.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'LabelAnnotation'\n        })\">LabelAnnotation</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'LabelAnnotation'\n        })\">LabelAnnotation</a></p>","types":["Object[]"],"optional":false,"nullable":false},{"name":"frameLabelAnnotations","description":"<p> Label annotations on frame level.  There is exactly one element for each unique label.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'LabelAnnotation'\n        })\">LabelAnnotation</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'LabelAnnotation'\n        })\">LabelAnnotation</a></p>","types":["Object[]"],"optional":false,"nullable":false},{"name":"faceAnnotations","description":"<p> Face annotations. There is exactly one element for each unique face.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'FaceAnnotation'\n        })\">FaceAnnotation</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'FaceAnnotation'\n        })\">FaceAnnotation</a></p>","types":["Object[]"],"optional":false,"nullable":false},{"name":"shotAnnotations","description":"<p> Shot annotations. Each shot is represented as a video segment.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'VideoSegment'\n        })\">VideoSegment</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'VideoSegment'\n        })\">VideoSegment</a></p>","types":["Object[]"],"optional":false,"nullable":false},{"name":"explicitAnnotation","description":"<p> Explicit content annotation.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'ExplicitContentAnnotation'\n        })\">ExplicitContentAnnotation</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'ExplicitContentAnnotation'\n        })\">ExplicitContentAnnotation</a></p>","types":["Object"],"optional":false,"nullable":false},{"name":"error","description":"<p> If set, indicates an error. Note that for a single <code>AnnotateVideoRequest</code>  some videos may succeed and some may fail.</p><p> This object should have the same structure as google.rpc.Status</p>","types":["Object"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"AnnotateVideoResponse","name":"AnnotateVideoResponse","type":"instance","description":"","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L485","resources":[],"examples":[],"params":[{"name":"annotationResults","description":"<p> Annotation results for all videos specified in <code>AnnotateVideoRequest</code>.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'VideoAnnotationResults'\n        })\">VideoAnnotationResults</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'VideoAnnotationResults'\n        })\">VideoAnnotationResults</a></p>","types":["Object[]"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"VideoAnnotationProgress","name":"VideoAnnotationProgress","type":"instance","description":"<p>Annotation progress for a single video.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L513","resources":[],"examples":[],"params":[{"name":"inputUri","description":"<p> Video file location in  <a href=\"https://cloud.google.com/storage/\">Google Cloud Storage</a>.</p>","types":["string"],"optional":false,"nullable":false},{"name":"progressPercent","description":"<p> Approximate percentage processed thus far.  Guaranteed to be 100 when fully processed.</p>","types":["number"],"optional":false,"nullable":false},{"name":"startTime","description":"<p> Time when the request was received.</p><p> This object should have the same structure as google.protobuf.Timestamp</p>","types":["Object"],"optional":false,"nullable":false},{"name":"updateTime","description":"<p> Time of the most recent update.</p><p> This object should have the same structure as google.protobuf.Timestamp</p>","types":["Object"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"AnnotateVideoProgress","name":"AnnotateVideoProgress","type":"instance","description":"<p>Video annotation progress. Included in the <code>metadata</code> field of the <code>Operation</code> returned by the <code>GetOperation</code> call of the <code>google::longrunning::Operations</code> service.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L530","resources":[],"examples":[],"params":[{"name":"annotationProgress","description":"<p> Progress metadata for all videos specified in <code>AnnotateVideoRequest</code>.</p><p> This object should have the same structure as <a ng-if=\"service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.parent}}/{{service.path.split('/').shift()}}/data_types',\n          method: 'VideoAnnotationProgress'\n        })\">VideoAnnotationProgress</a>\n\n        <a ng-if=\"!service.parent\" ui-sref=\"docs.service({\n          serviceId: '{{service.title.split('V')[0] + '/v' + service.title.split('V')[1]}}/data_types',\n          method: 'VideoAnnotationProgress'\n        })\">VideoAnnotationProgress</a></p>","types":["Object[]"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"Feature","name":"Feature","type":"instance","description":"<p>Video annotation feature.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L539","resources":[],"examples":[],"params":[],"exceptions":[],"returns":[]},{"id":"LabelDetectionMode","name":"LabelDetectionMode","type":"instance","description":"<p>Label detection mode.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L572","resources":[],"examples":[],"params":[],"exceptions":[],"returns":[]},{"id":"Likelihood","name":"Likelihood","type":"instance","description":"<p>Bucketized representation of likelihood.</p>","source":"packages\\video-intelligence\\src\\v1beta2\\doc\\doc_video_intelligence.js#L600","resources":[],"examples":[],"params":[],"exceptions":[],"returns":[]}],"path":"v1beta2/data_types.json","description":"\n          <table class=\"table\">\n            <thead>\n              <tr>\n                <th>Class</th>\n                <th>Description</th>\n              </tr>\n            </thead>\n            <tbody>\n              <tr ng-repeat=\"method in service.methods\" ng-if=\"method.name\">\n                <td>\n                  <a ui-sref=\"docs.service({ method: method.id })\" class=\"skip-external-link\">\n                    {{method.name}}\n                  </a>\n                </td>\n                <td>\n                  <span ng-bind-html=\"method.description\">\n                    {{method.description}}\n                  </span>\n                  <span ng-if=\"!method.description && method.name.includes('Request')\">\n                    The request for {{method.name}}.\n                  </span>\n                  <span ng-if=\"!method.description && method.name.includes('Response')\">\n                    The response for {{method.name}}.\n                  </span>\n                </td>\n              </tr>\n            </tbody>\n          </table>\n        ","id":"video-intelligence/v1beta2/data_types"}